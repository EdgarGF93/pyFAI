{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60736944-f728-4729-9275-0c55fa2b29ea",
   "metadata": {},
   "source": [
    "# Multiprocessing on GPU\n",
    "\n",
    "This is an extreme case where we will try how to use all resources of the computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd653b1-3040-4f08-9bfe-1b1178cfcde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCL devices:\n",
      "[0] NVIDIA CUDA: (0,0) NVIDIA RTX A5000, (0,1) Quadro M2000\n",
      "[1] Intel(R) OpenCL: (1,0) Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz\n",
      "[2] Intel(R) OpenCL: (2,0) Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60GHz\n",
      "[3] Intel(R) FPGA Emulation Platform for OpenCL(TM): (3,0) Intel(R) FPGA Emulation Device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import concurrent\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import shared_memory, Process, Value\n",
    "multiprocessing.set_start_method('spawn')\n",
    "mpctx = multiprocessing.get_context('spawn')\n",
    "import numpy\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "import pyFAI\n",
    "\n",
    "from silx.opencl import ocl\n",
    "from silx.opencl.codec.bitshuffle_lz4 import BitshuffleLz4\n",
    "import inspect\n",
    "from worker import Item\n",
    "MAIN_PROCESS = multiprocessing.parent_process() is None\n",
    "\n",
    "print(ocl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6aaba7-377d-4478-87b4-45b1b85c7ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#This cell contains the parameters for all the processing\n",
      "DEVICES= [(0,0),(0,1)]\n",
      "NWORKERS = 4\n",
      "FRAME_PER_FILE = 100\n",
      "NFILES = 100\n",
      "NBINS = 1000\n",
      "SHARED_NAME = 'shared_results'\n",
      "DETECTOR=\"Eiger_4M\"\n",
      "pathname = \"/tmp/big_%04d.h5\"\n",
      "pathmask = \"/tmp/big_????.h5\"\n",
      "dtype = \"float32\"\n",
      "array_shape = (NFILES, FRAME_PER_FILE, NBINS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import param\n",
    "print(inspect.getsource(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f58cd73-78b5-4b6e-a6cd-2d553d35f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def build_integrator(detector=DETECTOR):\n",
      "    geo = {\"detector\": detector, \n",
      "           \"wavelength\": 1e-10, \n",
      "           \"rot3\":0} #work around a bug https://github.com/silx-kit/pyFAI/pull/1749\n",
      "    ai = pyFAI.load(geo)\n",
      "    return ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from worker import build_integrator\n",
    "print(inspect.getsource(build_integrator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4a5604-4c2d-4bdb-aea8-63114f9e43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of files\n",
    "   \n",
    "def generate_one_frame(ai, unit=\"q_nm^-1\", dtype=\"uint32\"):\n",
    "    \"\"\"Prepare a frame with little count so that it compresses well\"\"\"\n",
    "    qmax = ai.array_from_unit(unit=unit).max()\n",
    "    q = numpy.linspace(0, qmax, 100)\n",
    "    img = ai.calcfrom1d(q, 100/(1+q*q))\n",
    "    frame = numpy.random.poisson(img).astype(dtype)\n",
    "    return frame\n",
    "\n",
    "def generate_files(img):\n",
    "    cmp = hdf5plugin.Bitshuffle()\n",
    "    filename = pathname%0\n",
    "    shape = img.shape\n",
    "    with h5py.File(filename, \"w\") as h:\n",
    "        ds = h.create_dataset(\"data\", shape=(FRAME_PER_FILE,)+shape, chunks=(1,)+shape, dtype=img.dtype, **cmp) \n",
    "        for i in range(FRAME_PER_FILE):\n",
    "            ds[i] = img + i%500 #Each frame has a different value to prevent caching effects\n",
    "    res = [filename]\n",
    "    for i in range(1, NFILES):\n",
    "        new_file = pathname%i\n",
    "        os.link(filename,new_file)\n",
    "        res.append(new_file)\n",
    "    return res\n",
    "\n",
    "# Create a set of files with dummy data in them:\n",
    "if not glob.glob(param.pathmask): \n",
    "    input_files = generate_files(generate_one_frame(build_integrator(DETECTOR)))\n",
    "else:\n",
    "    input_files = glob.glob(param.pathmask)\n",
    "    input_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a5845f-1ce6-41e9-9b1a-4ce532058d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_shared_array(shape, dtype=\"float32\", name=SHARED_NAME, create=False):\n",
      "    d_size = numpy.prod(shape)*numpy.dtype(dtype).itemsize\n",
      "    shm = shared_memory.SharedMemory(create=create, size=d_size, name=SHARED_NAME)\n",
      "    # numpy array on shared memory buffer\n",
      "    dst = numpy.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)\n",
      "    return dst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This is allows to create and destroy shared numpy arrays\n",
    "from worker import create_shared_array\n",
    "print(inspect.getsource(create_shared_array))\n",
    "\n",
    "def release_shared(name=param.SHARED_NAME):\n",
    "    shm = shared_memory.SharedMemory(name=name)\n",
    "    shm.close()\n",
    "    shm.unlink()  # Free and release the shared memory block\n",
    "\n",
    "if MAIN_PROCESS:\n",
    "    result_array = create_shared_array(param.array_shape, param.dtype, param.SHARED_NAME, create=MAIN_PROCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc73341d-85d8-4a60-9085-3a556312a8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def worker(rank, queue, shm_name, counter):\n",
      "    \"\"\"Function representing one worker, used in a pool of worker.\n",
      "    \n",
      "    :param rank: integer, index of the worker.\n",
      "    :param queue: input queue, expects Item with index and name of the file to process\n",
      "    :param shm_name: name of the output shared memory to put integrated intensities\n",
      "    :param counter: decremented when quits\n",
      "    :return: nothing, used in a process.\n",
      "    \"\"\"\n",
      "    #Start up the integrator:\n",
      "    ai = build_integrator(DETECTOR)\n",
      "    blank = numpy.zeros(ai.detector.shape, dtype=\"uint32\")\n",
      "    method = (\"full\", \"csr\", \"opencl\", DEVICES[rank%len(DEVICES)])\n",
      "    res = ai.integrate1d(blank, NBINS, method=method)\n",
      "    omega = ai.solidAngleArray()\n",
      "    engine = ai.engines[res.method].engine\n",
      "    print(res.method)\n",
      "    omega_crc = engine.on_device[\"solidangle\"]\n",
      "    engine = new_engine(engine, 512)\n",
      "   \n",
      "    gpu_decompressor = BitshuffleLz4(2000000, blank.size, dtype=blank.dtype, ctx=engine.ctx)\n",
      "    gpu_decompressor.block_size = 128\n",
      "    result_array = create_shared_array(array_shape, dtype, SHARED_NAME, create=False)\n",
      "    \n",
      "    while True:\n",
      "        item = queue.get()\n",
      "        print(f\"{rank}: got {item}\")\n",
      "        index = item.index\n",
      "        if index<0: \n",
      "            queue.task_done()\n",
      "            with counter.get_lock():\n",
      "                counter.value -= 1\n",
      "            return\n",
      "        with h5py.File(item.filename, \"r\") as h5:\n",
      "            ds = h5[\"data\"]\n",
      "            for i in range(ds.id.get_num_chunks()):\n",
      "                filter_mask, chunk = ds.id.read_direct_chunk(ds.id.get_chunk_info(i).chunk_offset)\n",
      "                if filter_mask == 0:\n",
      "                    print(f\"{rank}: process frame #{i}\")\n",
      "                    dec = gpu_decompressor(chunk)\n",
      "                    print(f\"{rank}: decompressed #{i}\")\n",
      "                    intensity = engine.integrate_ng(dec, solidangle=omega, solidangle_checksum=omega_crc).intensity\n",
      "                    print(f\"{rank}: integrated #{i}\")\n",
      "                    result_array[index, i,:] = intensity.astype(dtype)\n",
      "                    print(f\"{rank}: stored #{i}\")\n",
      "        queue.task_done()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from worker import worker\n",
    "print(inspect.getsource(worker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80957b0-fbe3-4692-8536-a56e321b5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pool(nbprocess, queue, shm_name, counter):\n",
    "    \"\"\"Build a pool of processes with workers, and starts them\"\"\"\n",
    "    pool = [Process(target=worker, name=f\"worker_{i:02d}\", args=(i, queue, shm_name, counter)) for i in range(nbprocess)]\n",
    "    for process in pool:\n",
    "        process.start()\n",
    "    return pool\n",
    "\n",
    "def end_pool(pool, queue):\n",
    "    \"\"\"Ends all processes from a pool by sending them a \"kill-pill\"\"\"\n",
    "    for process in pool:\n",
    "        queue.put(Item(-1, None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5d1cf8-a8c6-4e42-b9c3-34d008f25825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pool of workers\n",
    "queue = mpctx.Queue()\n",
    "counter = mpctx.Value(\"i\", param.NWORKERS)\n",
    "pool = [Process(target=worker, name=f\"worker_{i:02d}\", args=(i, queue, param.SHARED_NAME, counter)) for i in range(param.NWORKERS)]\n",
    "# pool=build_pool(NWORKERS, queue, SHARED_NAME, counter)\n",
    "# for idx, fn in enumerate(input_files):\n",
    "#     queue.put(Item(idx, fn))\n",
    "# for i in range(NWORKERS):\n",
    "#     queue.put(Item(-1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb18b69-77ac-442e-b624-1ecb5ec7d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAIN_PROCESS:\n",
    "    for p in pool: p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9ef84b-650b-4b8e-87df-104d6aea6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAIN_PROCESS:\n",
    "    for idx, fn in enumerate(input_files):\n",
    "        queue.put(Item(idx, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1491ab-d03c-4685-8adf-0b0de570a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(param.NWORKERS):\n",
    "    queue.put(Item(-1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4820f7-5662-48d1-9de7-cc13f6e0230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#release_shared(SHARED_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a91d75f-baf2-44df-b57b-2776f5ca5f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978b72f0-1ff1-4f19-ad5b-dcdd97e41851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584fc0b-6109-4b4e-a87b-b29ca3d87289",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37189a-622b-4ab1-978e-690f732eb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a7cb6-2884-4adc-b15b-bf0bf9455161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_nm^-1\n"
     ]
    }
   ],
   "source": [
    "w = worker(0, queue, SHARED_NAME, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f444b01d-8ff2-440d-8084-ced96e07bba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7fa2b5f7b670>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-72df8b647631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7fa2b5f7b670>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dumps(lambda x:2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbc6ac-86d2-4cf5-9176-bc63c96732a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
